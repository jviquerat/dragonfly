%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Experiments}

This chapter contains experiments on several technical details of algorithms. Most often, the goal is to compare the performances of slightly implementations on a set of reference environments.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Standard deviation generation for normal law (revision \codeinline{04cb588})}
%
%When using the normal law policy with on-policy algorithms such as \ppo, several options exist to generate the standard deviation vector, the policy network can output either the standard deviation or its logarithm. The impact of this choice is explored in this section. Here, the policy network is composed of a common trunk (one layer of size 64) followed two branches (each with one layer of size 64). The branch for the mean value ends with a $\tanh$ activation function, while the branch for the standard deviation ends either with a sigmoid activation (direct output of standard deviation) or a linear activation (output of the log of the standard deviation). In the latter case, the output is clipped to $[-20,0]$ before being exponentiated, meaning the standard deviation is roughly in $[\num{2e-9},1]$. The mean value being constrained in $[-1,1]$ by the $\tanh$ activation, this ensures a proper exploration of the domain, the actions being mapped to the physical bounds of the problem afterward. In figure \ref{fig:normal_stddev}, we compare the performance of the two approaches on several standard \gym and \mujoco environments. It appears in figure \ref{fig:normal_stddev} that, overall, the regular approach performs best on most environments except \codeinline{pendulum-v1}.
%
%\input{fig/stddev/normal_stddev}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Initializers (revision \codeinline{04cb588})}
%
%In figure \ref{fig:initializer}, we check the impact of the kernel initializations on the performance using \ppo. It does not seem to make much of a difference, although Lecun normal exhibits a slightly slower convergence.
%
%\input{fig/initialization/initialization}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Value network warmup with \ppo}
%
%The idea is to introduce a warmup phase at the beginning of training, where only the critic trains but not the actor (here for $20k$ steps). As shown in figure \ref{fig:ppo_value_warmup}, it does seem to help a little, but it is not a gamechanger.
%
%\input{fig/ppo_value_warmup/ppo_value_warmup}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Adam $\epsilon$ value}
%
%Some people claim that using higher-than-default Adam $\epsilon$ parameter is supposed to help regularize learning. Yet it was not found to be a critical choice in \cite{andrychowicz2020}, and in our case we even found that it could be detrimental (see figure \ref{fig:adam_epsilon}).
%
%\input{fig/adam_epsilon/ppo_adam_epsilon}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{SNN}
%
%Test self-normalizing neural networks, combining selu activation with normal initialization and alpha dropout (see paper)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Early stopping using KL-divergence with \ppo}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Activation function}
%
%tanh, relu, selu, gelu, swish, mish

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Clipped normal against squashed normal with \ppo}
%
%Must use linear/softplus activation and remove the action clipping and rescaling

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Bootstrapping}

We compare performance on the \ppo algorithm with and without the use of bootstrapping in figure \ref{fig:bootstrap}. There is a clear advantage in using bootstrapping in terms of learning speed and final performance. It is interesting to see that bootstrapping induces an initial lowering in score in the first steps of the \codeinline{lunarlandercontinuous-v2} environment, which also happens repeatedly in figure \ref{fig:gae_lunarlander}. This behavior remains to be explored.

\input{fig/bootstrap/bootstrap}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Value of \gae $\lambda$}

We compare the performance of \ppo + \gae with different values of the $\lambda$ parameter in figure \ref{fig:ppo_gae}. The usual range proposed in the original paper \cite{gae} is between $0.90$ and $0.99$. Here, we see that both environments behave similarly for $\lambda$ between $0.90$ and $0.98$, but react differently to $\lambda = 0.85$. Details on the \gae derivation are provided in section \ref{section:gae}.

\input{fig/gae/gae}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{\gae vs \cgae}

In figure \ref{fig:ppo_cgae}, we compare the performance of the regular \gae with that of \cgae presented in section \ref{section:gae}. As of now, more tests are needed, mostly with shorter episodes, where the effet of the \cgae correction could mostly make a difference.

\input{fig/cgae/cgae}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Entropy bonus}

Policy gradient methods can benefit from the use of an entropy bonus that prevents premature shrinking of the exploration. This method was originally proposed in \cite{williams1991} in the context of reinforcement learning, and re-introduced in deep reinforcement learning in \cite{a2c} and \cite{ppo}. The idea is to add an estimation of the policy entropy to the loss. For the policy gradient loss using advantage, for example, this yields:

\begin{equation}
	\mathcal{L}(\theta) = \expect{(s_t, a_t) \sim \pi_\theta} \Bigl[ \log \left( \pi_\theta (s_t, a_t) \right) A^{\pi_\theta} (s_t, a_t) + \beta \entropyÂ \left( \pi_\theta (s_t) \right) \Bigr].
\end{equation}

We see in figure \ref{fig:ppo_entropy} that, in the case of \codeinline{bipedalwalker-v3}, a well-tuned bonus allows to solve the environment, while in the context of \codeinline{lunarlandercontinuous-v3}, small bonuses make no difference, and only a too large bonus is detrimental to the agent.

\input{fig/entropy/entropy}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Time-awareness}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Random initial state}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Exploration in \tdt and \sac}

%Compare exploration rates on \codeinline{lunarlandercontinuous} for example

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{\sac without reparameterization trick}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Target network in \dqn}
%
%\dqn with and without target network

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Covariance matrix of normal policy}

The variance of the normal policy may be parameterized in different ways:

\begin{enumerate}
	\item a fixed isotropic variance $\sigma_0^2$ (hereafter denoted as constant),
	\item a learned isotropic variance $\sigma^2$ (hereafter denoted as isotropic),
	\item a learned diagonal covariance matrix $\V{\Sigma}_\text{diag}$ (hereafter denoted as diagonal),
	\item a learned full covariance matrix $\V{\Sigma}_\text{full}$ (not considered here).
\end{enumerate}

The performances of the different parameterizations are compared in figure \ref{fig:cov} using the \ppo algorithm.

\input{fig/covariance/covariance}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Parallel off-policy agent}

Off-policy agents typically train once per step, which make them slower to run than on-policy agents (regardless of final performance). Here we  compare two different approaches:

\begin{enumerate}
	\item sequential environment with one update per step,
	\item sequential environment with \codeinline{n_stp_unroll} updates every \codeinline{n_stp_unroll} steps.
\end{enumerate}

The performances of the different approaches are compared in figure \ref{fig:n_stp_unroll} using the \tdt algorithm. Regarding pure CPU performance, the baseline run takes approximately $530$ seconds, going down to $440$ seconds for \codeinline{n_stp_unroll}$\, = 16$, and $390$ seconds for \codeinline{n_stp_unroll}$\, = 64$.

\input{fig/n_stp_unroll/n_stp_unroll}
