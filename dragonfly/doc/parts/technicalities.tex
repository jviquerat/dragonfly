%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Technicalities}

This chapter regroups several technical notes on the implementations and algorithms.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Expected grad-log-probability lemma}

Suppose a random variable $x$ following a parameterized p.d.f. $\pi_\theta$. Then the expected value of the gradient of its log-probability is equal to $0$:

\begin{equation}
\label{eq:eglp}
	\expect{x \sim \pi_\theta} \left[ \nabla_\theta \log \pi_\theta(x) \right] = 0.
\end{equation}

The proof starts with the simple following statement:

\begin{equation*}
	\int_x \pi_\theta(x) = 1,
\end{equation*}

and therefore:

\begin{equation*}
	\nabla_\theta \int_x \pi_\theta(x) = 0.
\end{equation*}

Then, using the log-prob trick:

\begin{equation*}
\begin{aligned}
	\nabla_\theta \int_x \pi_\theta(x) 	&= \int_x \nabla_\theta \pi_\theta(x)\\
								&= \int_x \pi_\theta(x) \nabla_\theta \log \pi_\theta(x)\\
								&= \expect{x \sim \pi_\theta} \left[ \nabla_\theta \log \pi_\theta(x) \right],
\end{aligned}
\end{equation*}

which is therefore equal to $0$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Log-probability of squashed gaussian in soft actor-critic}

In the \sac algorithm, the infinite support Gaussian p.d.f. $\bm{\mu}(\bm{u})$ with $\bm{u} \in \mathbb{R}^d$ is squashed using the hyperbolic tangent function, leading to actions $\bm{a} = \tanh (\bm{u}) \in [-1,1]^d$. If $\bm{u}$ is a random variable following p.d.f. $\bm{\mu}$, and $\bm{a} = h(\bm{u})$ with $h$ bijective and differentiable, then $\bm{a}$ is a random variable following p.d.f. $\bm{\pi}$:

\begin{equation*}
	\bm{\pi}(\bm{a}) = \bm{\mu}\left( h^{-1}(\bm{a}) \right) \left| \det \left( \left. \frac{dh^{-1}(\bm{z})}{d\bm{z}} \right|_{\bm{z} = \bm{a}} \right) \right|.
\end{equation*}

Here, $\bm{a} = \tanh (\bm{u})$ leads to:

\begin{equation*}
	\left. \frac{d\tanh^{-1}(\bm{z})}{d\bm{z}} \right|_{\bm{z} = \bm{a}} = \text{diag} \left( \frac{1}{1-a_i^2} \right)_{i\in[1,N]} = \text{diag} \left( \frac{1}{1-\tanh^2(u_i)} \right)_{i\in[1,N]},
\end{equation*}

and therefore:

\begin{equation}
\label{eq:sac_squashed_gaussian}
	\bm{\pi}(\bm{a}) = \bm{\mu}\left( \bm{u} \right) \prod_{i=1}^{N} \frac{1}{1 - \tanh^2(u_i)}.
\end{equation}

The log-probability is easily computed:

\begin{equation}
\label{eq:sac_squashed_gaussian_logprob}
	\log \bm{\pi}(\bm{a}) = \log \bm{\mu}\left( \bm{u} \right) - \sum_{i=1}^{N} \log \left( 1 - \tanh^2(u_i) \right).
\end{equation}

Although easy to implement, numerical evaluation of expression (\ref{eq:sac_squashed_gaussian_logprob}) can lead to undefined values for large absolute values of $u_i$. The OpenAI implementation \cite{spinningup} proposes another form of this expression that leads to improved numerical stability\footnote{\url{https://github.com/openai/spinningup/blob/master/spinup/algos/tf1/sac/core.py} line 54}:

\begin{equation}
\label{eq:logprob_openai}
\begin{aligned}
	\log \left( 1 - \tanh^2(u) \right) 	&= \log \left( \frac{4}{\left(\exp^u + \exp^{-u}\right)^2} \right)\\
							&= 2 \left( \log(2) - \log \left( \exp^u + \exp^{-u} \right) \right)\\
							&= 2 \left( \log(2) - u - \log \left( 1 + \exp^{-2u} \right) \right)\\
							&= 2 \left( \log(2) - u - \text{softplus} (-2u) \right).
\end{aligned}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Cross-entropy and KL divergence}

The Kullback-Leibler distance, also called KL divergence or relative entropy, measures the distance between two distributions $f$ and $g$. For a random variable $X = (x_1, ..., x_n)$ defined on a support $\chi$, the KL divergence is defined as:

\begin{equation}
\label{eq:kl_div}
\begin{aligned}
	\mathcal{D}(f,g) 	&= \expect{x \sim f} \left[ \log \frac{f(x)}{g(x)} \right]\\
					&= \int_{x} f(x) \log f(x) - \int_{x} f(x) \log g(x)
\end{aligned}
\end{equation}

The first term of second line in (\ref{eq:kl_div}) is the opposite of the entropy of $f$, denoted $\mathcal{H}(f)$:

\begin{equation}
\label{eq:entropy}
\begin{aligned}
	\mathcal{H}(f) 	&= - \expect{x \sim f} \left[ \log f(x) \right]\\
				&= - \int_{x} f(x) \log f(x).
\end{aligned}
\end{equation}

Minimizing the KL distance between a target distribution $f$ and a parameterized distribution $g_\theta$ is equivalent to minimize their cross-entropy $\mathcal{H} (f,g_\theta)$:

\begin{equation}
\label{eq:cross_entropy}
\begin{aligned}
	\mathcal{H}(f,g_\theta) 	&= \mathcal{H}(f) + \mathcal{D}(f,g_\theta)\\
						&= - \int_{x} f(x) \log g_\theta(x).
\end{aligned}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Generalized advantage estimate}
\label{section:gae}

This section presents a light approach of the \gae, which is used by default for policy gradient methods in the library (it heavily borrows from Julien Vitay's explanation, see \url{https://julien-vitay.net/deeprl/3.2-ActorCritic.html} section 5.0.3). First, let's remind that the policy gradient can take the following general form:

\begin{equation}
	\nabla_\theta J(\theta) = \expect{(s_t,a_t) \sim \pi_\theta} \left[ \nabla_\theta \log \pi_\theta (s_t,a_t) \psi_t \right],
\end{equation}

where $\psi_t$ can be any of the following:

\begin{enumerate}
	\item $\psi_t = \sum_{t=0}^T r_t = G_\tau$, the full trajectory return;
	\item $\psi_t = \sum_{t=t}^T r_t = G_t$, the return taken from current step;
	\item $\psi_t = Q(s_t, a_t)$, the Q-value function;
	\item $\psi_t = A(s_t, a_t)$, the advantage function;
	\item $\psi_t = r_t + \gamma V(s_{t+1}) - V(s_t)$, the one-step TD residual;
	\item $\psi_t = r_t + \gamma r_{t+1} + ... + \gamma^n V(s_{t+n}) - V(s_t)$, the $n$-step TD residual;
\end{enumerate}

In order to build the GAE, we define the a class of advantage function estimators based on TD residuals:

\begin{equation}
	A^{(n)}_t = r_t + \gamma r_{t+1} + ... + \gamma^n V(s_{t+n}) - V(s_t),
\end{equation}

so $A^{(1)}_t$ is the standard one-step TD residual, etc. The tradeoff is that, for small values of $n$, $A^{(n)}_t$ will have large bias and small variance, while for large values of $n$, it will have low bias but high variance. We can show that $A^{(n)}$ can be expressed in terms of the temporal residual $\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)$ only:

\begin{equation}
\begin{aligned}
	A^{(n)}_t 	&= r_t + \gamma r_{t+1} + ... + \gamma^n V(s_{t+n}) - V(s_t)\\
			&= r_t + \textcolor{red}{\gamma V(s_{t+1})} - V(s_t) + \textcolor{red}{\gamma} \left( r_{t+1} + \gamma V(s_{t+2}) \textcolor{red}{- V(s_{t+1})} \right) + ...\\
			&= \delta_{t} + \gamma \delta_{t+1} + ...\\
			&= \sum_{l=0}^{n-1} \gamma^l \delta_{t+l}
\end{aligned}
\end{equation}

Then, the \gae advantage estimate is defined as a combination of the $A^{(n)}_t$. Let's try to recover the original expression:

\begin{equation}
\label{eq:gae}
\begin{aligned}
	A^{\gae}_t 	&= (1-\lambda) \left( A^{(1)}_t + \lambda A^{(2)}_t + ... + \lambda^{n-1} A^{(n)}_t \right)\\
				&= (1-\lambda) \left( \delta_t + \lambda \left( \delta_t + \gamma \delta_{t+1} \right) + ... + \lambda^{n-1} \left( \delta_t + \gamma \delta_{t+1} + ... + \gamma^{n-1} \delta_{t+n-1} \right) \right)\\
				&= (1-\lambda) \left( \delta_t \left(1 + ... + \lambda^{n-1} \right) + \gamma \lambda \delta_{t+1} \left(1 + ... + \lambda^{n-2} \right) + ... + \gamma^{n-1} \lambda^{n-1} \delta_{t+n-1} \right)\\
				&= (1-\lambda) \left( \frac{1 - \lambda^n}{1 - \lambda} \delta_t + \frac{1 - \lambda^{n-1}}{1 - \lambda} \gamma \lambda \delta_{t+1} + ... + \gamma^{n-1} \lambda^{n-1} \delta_{t+n-1} \right)\\
				&= \left(1-\lambda^{n} \right) \delta_t + \gamma \lambda \left(1-\lambda^{n-1} \right) \delta_{t+1} + ... + \gamma^{n-1} \lambda^{n-1} (1 - \lambda) \delta_{t+n-1}\\
				&= \sum_{l=0}^{n-1} (\gamma \lambda)^l \left(1 - \lambda^{n-l}\right) \delta_{t+l}\\
				&= \sum_{l=0}^{n-1} \gamma^l \left(\lambda^l - \lambda^n\right) \delta_{t+l}.
\end{aligned}
\end{equation}

We see that 

\begin{equation}
	A^{\gae}_t \underset{n \rightarrow \infty}{\longrightarrow} A^{\gae}_{t, \infty} = \sum_{l=0}^{n-1} (\gamma \lambda)^l \delta_{t+l},
\end{equation}

which is the expression proposed in the original paper \cite{gae}. This expression verifies a recursive relation, which is very useful when it comes to implementing it:

\begin{equation}
\begin{aligned}
	A^{\gae}_{t, \infty} 	&= \delta_t + \sum_{l=1}^{n-1} (\gamma \lambda)^l \delta_{t+l}\\
					&= \delta_t + \gamma \lambda \sum_{l=0}^{n-2} (\gamma \lambda)^l \delta_{t+l+1}\\
					&= \delta_t + \gamma \lambda A^{\gae}_{t+1, \infty}.
\end{aligned}
\end{equation}

Yet, for the non-asymptotic form, the same relation does not hold. We develop the non-asymptotic expression as follows:

\begin{equation}
\begin{aligned}
	A^{\gae}_t 	&= \sum_{l=0}^{n-1} (\gamma \lambda)^l \delta_{t+l} - \lambda^n \sum_{l=0}^{n-1} \gamma^l \delta_{t+l} \\
				&= A^{\gae}_{t,\infty} - \lambda^n \sum_{l=0}^{n-1} \gamma^l \delta_{t+l}\\
				&= A^{\gae}_{t,\infty} - \lambda^n A^{\gae}_{t,c},
\end{aligned}
\end{equation}

where $A^{\gae}_{t,c}$ is the corrective term for non-asymptotic form. It is easy to see that $A^{\gae}_{t,c}$ verifies the same recursion relation than $A^{\gae}_{t,\infty}$:

\begin{equation}
	A^{\gae}_{t,c} 	= \delta_t + \gamma A^{\gae}_{t+1,c},
\end{equation}

and therefore the two terms $A^{\gae}_{t,\infty}$ and $A^{\gae}_{t,c}$ can be computed separately and then assembled together. For future reference, we call this expression \cgae, standing for corrected generalized advantage estimate.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{The Bellman equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\section{Importance sampling and its relation to surrogate loss}