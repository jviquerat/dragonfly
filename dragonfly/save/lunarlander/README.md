## `lunarlander-v2` (discrete)

The agent learns to land on a landing pad using 4 discrete actions.

<p align="center">
  <img width="300" alt="" src="bad.gif">
  <img width="300" alt="" src="good.gif">
</p>

Here is a resolution of the discrete environment with PPO, using buffer-based training:

<p align="center">
  <img width="700" alt="" src="ppo_buffer_discrete.png">
</p>

Below is a resolution of the continuous environment, also with PPO:

<p align="center">
  <img width="700" alt="" src="ppo_buffer_continuous.png">
</p>
